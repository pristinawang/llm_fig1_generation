doc_id,pdf_url,motivation,Motivations (select),Done
1,http://ieeexplore.ieee.org/document/6853739/,Unspecified,Unspecified,TRUE
2,https://www.isca-archive.org/interspeech_2018/huang18b_interspeech.html,"""Emotion recognition has been attracting increasing attention recently, owing to its essential role in human behavioral signal processing""","Affective computing/HCI, Prior work",TRUE
3,https://arxiv.org/pdf/2203.08810.pdf,"""Speech emotion recognition (SER) has gained enormous interest in many diverse applications such as smart virtual assistants [1], medical diagnoses [2, 3], and education [4].""","HCI: Chatbots, Other / misc",TRUE
4,https://www.isca-archive.org/interspeech_2012/sun12c_interspeech.html,"No motivation of SER, motivation is drawing from other SER work: ""While the majority of traditional research in emotional speech recognition has utilized a single database for analysis, it is becoming clear that the lack of sufficiently large databases with varied emotion types is a significant hindrance to the design of a robust emotion classification system.""",Prior work,TRUE
5,https://epublications.marquette.edu/cgi/viewcontent.cgi?article=1008&context=data_drdolittle,"""This task has application to a number of important areas, including security systems, lie detection, video games and psychiatric aid, among others.""","Healthcare: Mental health, Other / misc, Security",TRUE
6,http://bth.diva-portal.org/smash/get/diva2:1460935/FULLTEXT01,"""Lesions in the brain getting in the way of thevspeech program can cause failures in the production of emotive speech at prosodic or segmental levels....In many disorders, emotive speech is sensitive to
disease progression and recovery. In other words, it is a good candidate biomarker to be included into the panel of others that are taken into account.""",Healthcare: Neurological disorders,TRUE
7,https://arxiv.org/pdf/2305.19184.pdf,"""Speech signals carry rich information on an individual’s emotional states, expressed through both paralinguistic and semantic cues.""",Affective computing/HCI,TRUE
8,https://www.isca-archive.org/interspeech_2007/inanoglu07_interspeech.html,"""The ability to add emotions to synthesised neutral speech has
become a high priority for unit-selection frameworks which require very large training corpora in order to generate highly intelligible and natural outputs.""",,TRUE
9,http://ieeexplore.ieee.org/document/6163948/,"A long-term objective of text-to-speech (TTS) synthesis is to make synthetic speech as expressive as natural speech. In order to attain that goal, the speech produced must be emotionally congruent with the underlying textual input, at least to the extent that such congruence is perceptually salient. Given the current state of emotion detection from text, this is not yet within reach. For all practical purposes, however, it may be sufficient to avoid synthesizing speech with a grossly inappropriate emotional quality.",Other / misc,TRUE
10,https://ieeexplore.ieee.org/document/9186776/,"""Nowadays when some aspects of life are intertwined with the virtual world, it is increasingly important for the computer to understand not only what the human wants but also how the human “feels”, via their facial expressions, so that the machine is able to produce a meaningful, pleasant feedback.""
""Furthermore, the audio recording carries not only the contextual sound units (phonemes), but also emotions of the speaker reflected in speed or intensity of her speech. Hence, it is beneficial for the computer to comprehend emotional states of the speaker, for instance, to make a joke when it perceives the user is happy, in an imaginary mutual human-machine conversation.""",HCI: Chatbots,TRUE
11,https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.html,"""Understanding human emotion is a crucial step towards better human-computer interaction [1, 2]. Most studies on
speech emotion recognition are centered on young-adult people,
mainly originating from English-speaking countries."" [HCI unspecified, limitations in prior work]","HCI: Chatbots, Affective computing/HCI",TRUE
12,https://www.isca-archive.org/interspeech_2020/chen20b_interspeech.html,"Understanding user intention is an essential factor in enriching user experience and it has become a hot topic in both research and industry areas. Just as people communicate with each other, speech plays a crucial role in capturing the explicit content messages and even the underlying intentions. That is because speech contains rich linguistic and para-linguistic information conveys the implicit information such as emotions [1]. Current techniques (such as [2]) have achieved high accuracy of recognizing the content from the speech utterance; however, it is not sufficient for HCI systems to fully understand the purpose and intention of the speaker. Para-linguistic information, especially emotions, can help HCI systems capture the real purpose and right feeling of speakers, thereby giving a better response. As a result, speech emotion recognition (SER) has received increasing attention.
[1] M. M. H. E. Ayadi, M. S. Kamel, and F. Karray, “Survey on
speech emotion recognition: Features, classification schemes, and
databases,” Pattern Recognition, vol. 44, no. 3, pp. 572–587,
2011.",HCI: Chatbots,TRUE
13,https://opus.bibliothek.uni-augsburg.de/opus4/files/79708/TAFFC3021015.pdf,,,TRUE
14,https://arxiv.org/pdf/2305.12263.pdf,Depression detection,Healthcare: Mental health,TRUE
15,https://www.isca-archive.org/eurospeech_2001/amir01_eurospeech.html,"Various recent studies have dealt with characterization of
emotional speech and automatic classification of emotional
utterances [1,2,3,4].",Prior work,TRUE
16,http://arxiv.org/pdf/2305.16065.pdf,"Speech Emotion Recognition (SER) has rapidly developed over the past two decades, and the research on the corpora, features, algorithms, and training models has boomed [1]. ...Although some researchers have also attempted to translate this approach into the wild, e.g., in-car voice systems [4], it is still rare to see SER applications in our daily lives. One of the major reasons is that the majority of SER research uses human annotation, i.e., gold-standard manual transcripts. In contrast, even for the ‘lab’ emotion corpora (e.g., IEMOCAP), transcripts from a state-of-the-art Automatic Speech Recognition (ASR) system can result in Word Error Rates (WERs) higher than 35%, depending on the emotion label [5]. This means that very few of the findings obtained in the lab can be replicated in the wild.",Prior work,TRUE
17,https://www.isca-archive.org/interspeech_2019/sahu19_interspeech.html,"It has applications in several
fields including building intelligent voice-assistants, psychiatry,
analysis of human interaction and other behavioral studies [1].
Affect recognition or emotion recognition is a well-researched
field and the results demonstrate that using speech features does
a better job at predicting arousal levels (intensity) than valence
(pleasantness) level of the utterance.","HCI: Chatbots, Affective computing/HCI, Healthcare: Mental health",TRUE
18,https://www.isca-archive.org/interspeech_2010/sanchez10_interspeech.html,"""we are interested in predicting the emotional status of 911 emergency-hotline
callers""","Call center / call screening, HCI: Chatbots, Other / misc",TRUE
19,https://opus.bibliothek.uni-augsburg.de/opus4/files/67139/i00_1665.pdf,"""In a call-center, such a system should be able to determine in a critical phase of the dialogue if the call should be passed over to a human operator.""",Call center / call screening,TRUE
20,https://arxiv.org/pdf/2207.02104.pdf,"However, the majority of this cross-corpus work has been
cross-lingual and occasionally both cross-lingual and crossage. In the past this has been arguably necessary given the
sparsity of emotional databases. But now with the introduction of larger datasets, such as the MOSEI database which
has around 65 hours of natural emotion data [20], more effort can be put on cross-corpus research focussing on a single
language and a single age group.",Prior work,TRUE
21,https://www.isca-archive.org/interspeech_2023/zhang23g_interspeech.html,"""Emotion plays a crucial role in our daily communication and emotion recognition finds applications in different domains like customer services [1], social robots and dialogue systems. In recent years, multimodal emotion recognition (MER) has attracted increasing attention.""","HCI: Chatbots, Affective computing/HCI, Other / misc",TRUE
22,https://www.isca-archive.org/interspeech_2008/jones08_interspeech.html,"Entertainment robots have found there way into the home.
Most common are robot pets [1] that can be controlled
remotely or programmed with simple actions [2]. In the 2004
film I, Robot, an android called Sonny interrupts a row
between the main characters, saying Excuse me. I note there
are elevated stress patterns in your speech. Although much of
the autonomous intelligence and abilities for complex
communications may be some way off for consumer robots,
recognising stress and emotion in the human voice is possible.
Acoustic emotion recognition [3] or voice stress analysis [4]
offers a range of commercial benefits. Call centres can track
the conversations between callers and agents for emotive
events to help agents better support caller frustrations, aid
agent training and detect fraudulent insurance claims [5]. Incar systems can recognise driver emotion and react to drowsy
or angry drivers to help them drive more safely [6-7].
Computer games can recognise player emotion and adapt game
play to maintain active engagement without stressing the
player with over challenging game play [8-9].","HCI: Chatbots, Other / misc",TRUE
23,https://www.isca-archive.org/eurospeech_2003/black03_eurospeech.html,,,TRUE
24,https://ieeexplore.ieee.org/document/8276251/,"In affective computing field, emotion recognition from speech plays a very important role, and has received much attention over the past few decades. Speech emotion recognition aims to recognize emotions from speech into the following categories, e.g., neutral, happiness, sadness and surprise [1]. It has been proven useful in many applications which require human-machine interaction, e.g., in-car board system, diagnostic tool for therapists, automatic translation systems, computer tutorial applications [1], [2], [3]","Affective computing/HCI, Healthcare: Mental health, Other / misc",TRUE
25,https://arxiv.org/pdf/2005.01400.pdf,"Intro: ""Deep neural networks trained in a supervised manner are a popular contemporary choice for various speech related tasks such as automatic speech recognition (ASR), emotion recognition and age/gender recognition. However they are a double-edged sword by virtue of providing extremely good performance given that large scale annotated data is available, which is usually expensive""
RW: ""Audiovisual emotion recognition has also seen a significant amount of recent research efforts. Automatic affect recognition has a variety of applications in various fields; from detecting depression [41], to more emotionally relevant advertising [42], [43], [44].""",Prior work,TRUE
26,https://arxiv.org/pdf/2312.16180,"""Real-time speech-emotion models can help to improve human-computer interaction experiences, such as voice
assistants [1, 2] and facilitate health/wellness applications such as health diagnoses [3, 4].""","HCI: Chatbots, Healthcare: Mental health",TRUE
27,https://www.isca-archive.org/interspeech_2009/mower09_interspeech.html,"""These models could lead to computer agents and robots that more naturally and functionally blend into human society""",Affective computing/HCI,TRUE
28,https://opus.bibliothek.uni-augsburg.de/opus4/files/76685/vlasenko08_interspeech.pdf,"Detecting non-lexical or paralinguistic cues from speech is one of the major challenges in the development of usable human machine interfaces (HMI). Notable among these cues are the
universal categorical emotional states (e.g. anger, boredom, disgust, fear, joy, neutral, sadness, etc.) and/or level of interest (neutrality, interest, curiosity), prevalent in day-to-day scenarios. Knowing such emotional states and/or levels of interest can help adjust system responses so that the user of such a system can be more engaged and have a more effective interaction with the system.",Affective computing/HCI,TRUE
29,http://link.springer.com/10.1007/978-3-319-32213-1_13,"""Human emotions can be understood by the intelligent machine, but information will not be successfully delivered correctly if the emotion is mis- interpreted...If the computer is given the ability to identify human emotions in the same way as a human does, communication is bound to be more effective.""",Affective computing/HCI,TRUE
30,https://arxiv.org/pdf/2104.07072.pdf,"""Human-machine interaction is constantly evolving towards the use of more natural interfaces, like speech. Still the key difference between human-human and human-machine communication is the ability of humans to recognize the emotion of their conversation peers and modify their communication strategy based on that.""",Affective computing/HCI,TRUE
31,https://www.isca-archive.org/interspeech_2007/gupta07_interspeech.html,"""Over the last decade, enterprises have started to use human operated call-centers to provide improved services to their customers...So the supervisors monitor a subset of calls and identify if any of them had extreme emotional characteristics (such as happy or angry moods)""",Call center / call screening,TRUE
32,https://arxiv.org/pdf/2404.06702,"""To investigate what is learnt by LEAF, we train LEAF on three of the tasks where its performance has been perviously reported: emotion recognition, keyword spotting, and language identification [6]. The tasks were chosen to represent a range of speech processing applications as well as prediction accuracies ranging from quite high to somewhat low (refer Table 1).""",Prior work,TRUE
33,https://www.aclweb.org/anthology/2020.acl-demos.31.pdf,"""Dialog systems or chatbots, both text-based and multi-modal, have received much attention in recent years, with an increasing number of dialog systems in both industrial contexts such as Amazon Alexa, Apple Siri, Microsoft Cortana, Google Duplex, XiaoIce...The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce (Zhou et al., 2018). Rather than promoting “an AI
companion with an emotional connection to satisfy the human need for communication, affection, and social belonging” (Zhou et al., 2018), ADVISER helps develop dialog systems that support users in efficiently fulfilling concrete goals, while at the same time considering social signals such as emotional states and engagement levels so as to remain friendly and likeable.""",Affective computing/HCI,TRUE
34,http://ieeexplore.ieee.org/document/7953131/,"""Recognizing the emotions expressed in a speech signal, as well as in other modes, is an hard task. It is characterized by a level of subjectivity in defining and perceiving an emotion, as well as by a lack of a univocal definition of standard descriptors for each specific emotion [1, 2].""",Prior work,TRUE
35,https://opus.bibliothek.uni-augsburg.de/opus4/files/71710/oates19_interspeech.pdf,"""With the rapid rise of speech emotion recognition (SER) research and its imminent advance into industry applications """,Prior work,TRUE
36,https://www.isca-archive.org/eurospeech_2003/auberge03_eurospeech.html,"""In such a view, the verbal communication needs a coherent affective processing in order to be adapted to the situation. It would mean in particular that not to choose to control the affective speech information in synthesis and recognition implies not only naturalness or agreement lacks, but above that, it perturbs the goals of the interaction. The interest for affective technologies is increasing [3], as well for the conversational chatterbots (Embodied Conversational Interface Agents of Cassel, Believable Social and Emotional Agents of Bryan Loyall, Affective Agents of Picard…)""","Affective computing/HCI, HCI: Chatbots",TRUE
37,http://arxiv.org/pdf/2305.12540.pdf,"""Nowadays, modern AI-based conversational agents employing ASR are also equipped with emotional awareness using SER to improve the overall user experience. These agents are indispensable in real-world applications such as call centers, automotive voice assistants, and voice-enabled home automation systems.""","Affective computing/HCI, Call center / call screening",TRUE
38,https://ieeexplore.ieee.org/document/8288569/,"MOOD disorder is a kind of mental illness which severely affects how you feel and think...As a mood is an emotional state, emotion expression or perception are relevant to mood disorder, and has been used for mood disorder detection [24]",Healthcare: Mental health,TRUE
39,https://opus.bibliothek.uni-augsburg.de/opus4/files/67303/67303.pdf,"mental health

""assess depression severity using speech""",Healthcare: Mental health,TRUE
40,http://ieeexplore.ieee.org/document/4960547/,"misc list (but also includes motivation for speech use in HMI due to inexpensive & easy access to data)

""Research in the recognition of human emotion is one of growing research fields in human-machine interface (HMI) and affective computing [1]. Emotion recognition can be achieved
by analyzing various modalities: speech and facial expressions [2], [3], gesture and body language [4] and bio information such as electrocardiogram, electromyography, electrodermal activity, skin temperature, blood volume pulse and
respiration [5]. Compared to other modalities, speech signal can be obtained more easily and inexpensively. For this
reason, it has a wider range of HMI applications: a service
robot that responds to the owner’s emotion, a computer game
that controls the game status by game-player’s emotion, and
an audio response system of the call center that automatically
connects the customer to the expert counsellor if the customer
is angry.""","Affective computing/HCI, Other / misc",TRUE
41,https://www.isca-archive.org/interspeech_2006/burkhardt06_interspeech.html,"Anticipating frustration / difficulty / anger in use

Anger detection is a topic that is gaining more and more attention with voice portal carriers, as it can be useful for quality measurement and empathic dialog strategies [1, 2]. In the context of customer care voice portals it can be helpful to detect potential problems that arise from a unsatisfactory course of interaction in order to help the customer by either offering the assistance of human operators or trying to react with appropriate dialog strategies.","Affective computing/HCI, Other / misc, HCI: Chatbots",TRUE
42,https://www.isca-archive.org/interspeech_2009/vlasenko09_interspeech.html,"The importance of human behavior based dialog strategies in
human machine interaction (HMI) lies in existing limitations
of automatic speech recognition technology. Current state-ofthe-art Automatic Speech Recognition (ASR) approaches still
cannot deal with flexible, unrestricted user’s language and emotional prosody colored speech[1]. Therefore, problems caused
by misunderstanding a user who refuses to follow a predefined,
and usually restricting, set of communicational rules seems to
be inevitable.
In the domain of human-machine interaction [2], we witness the rapid increase of research interest in affected user behavior. However, some aspects of affected user behavior during
HMI still turns out to be a challenge for developers of Spoken
Dialog Systems (SDS).","HCI: Chatbots, Affective computing/HCI",TRUE
43,https://ieeexplore.ieee.org/document/10163251/,"Research on emotion recognition can advance many applications, like distance education [30], social robots [31], video games[32], affective mirrors[33], and many others[34]. Despite the substantial advances in this area, speech emotion recognition

One of
which is the design of outstanding features that best reflect the
emotional content and should be robust against other properties
of speakers, like identity, genders, ages, etc. [26], [38], [39].","Affective computing/HCI, Other / misc, Prior work",TRUE
44,https://arxiv.org/pdf/2103.02993.pdf,"Automatic affect recognition is a vital component in human-tohuman communication affecting our social interaction, perception among others [1]. In order to accomplish a natural interaction between human and machine, intelligent systems need to recognise the emotional state of individuals. However, the task is challenging, as human emotions lack of temporal boundaries and different individuals express emotions in different ways [2]. In addition, emotions are expressed through multiple modalities.",Affective computing/HCI,TRUE
45,https://www.isca-archive.org/interspeech_2022/hu22e_interspeech.html,"Speech is the most convenient and efficient way of human communication, and the emotion information contained in speech plays a vital role in communication. Enabling machines to speak, think, and feel like humans has long been pursued in the field of artificial intelligence. The research of SER will promote the realization of this goal",Affective computing/HCI,TRUE
46,https://linkinghub.elsevier.com/retrieve/pii/S0167639323000894,"Speech emotion classification plays an important role in human– computer interaction, which has been widely used for various applications including robot interfaces, audio surveillance, web-based E-learning, call centers, carboard systems, computer games (Kerkeni et al., 2019). Since speech is often regarded to convey the human mood and emotion, it can be used to determine the emotional state (Tuncer et al., 2021). Therefore, various speech emotion classification systems have been proposed.","HCI: Chatbots, Affective computing/HCI, Other / misc",TRUE
47,https://arxiv.org/pdf/2302.10536,,,TRUE
48,https://opus.bibliothek.uni-augsburg.de/opus4/files/45055/45055.pdf,"None - launches right into ""deep learning has shown great potential for SER""",Unspecified,TRUE
49,https://www.isca-archive.org/eurospeech_2003/rahurkar03_eurospeech.html,"The
results suggest a important step forward in establishing an
effective processing scheme for developing generic
models of neutral and emotional speech. ...would take us a step closer to a generic processing sequence for obtaining models of new emotions in a fixed speaker environment of new input speakers for voice telephony or interactive systems. Another motivation for addressing emotional/stressful speech classification is to better understand physical speech modeling and associated speech feature estimation procedures....This property is especially useful here
because we believe that when a person is under stress,
physiological changes occur in their vocal fold movement,
which modulates airflow in the vocal tract. These changes
in the airflow properties are perceived by listener as
emotional content.
Such an emotion classification system could not only
be used to increase robustness in speech recognition it
could also contribute to improved systems for to handhelds, interactive books, and intelligent dialogue
systems. These are some of the many areas where such a
system can play a vital role.","Other / misc, HCI: Chatbots",TRUE
50,https://digitalcommons.fairfield.edu/cgi/viewcontent.cgi?article=1182&context=engineering-facultypubs,"Speech emotion recognition, referring to the process of detecting the emotional state of a speaker, has become a very active research topic in the affective computing field and has had a wide range of applications recently. Automatic emotion recognition from speech largely depends on the effectiveness of the speech features used for classification. Due to the subtleties of human emotion, how to extract discriminative and affect-salient features from the speech signals is still one of the important research topics [1, 2].",Affective computing/HCI,TRUE
51,https://www.isca-archive.org/interspeech_2017/zhang17b_interspeech.html,"In recent years, emotion recognition has become more and more important in the research topic area and focus is being placed on classifying emotion in audio signals [2, 3]. Emotion recognition is seen as critical in Human Computer Interface (HCI) when it comes to applications in the domains of software engineering, website customization, education and gaming, for it helps the machine better understand humans [4]. The aim of this research is speech emotion recognition in human-to-human dialogues.","Affective computing/HCI, Other / misc",TRUE
52,https://ieeexplore.ieee.org/document/8461451/,"No mention of the specific task of SER, just motivation behind privacy-preservation for these models",Security,TRUE
53,http://arxiv.org/pdf/2206.13101.pdf,"Emotions, an “implicit channel” that transmits explicit messages [1], play a vital role in human communication. Speech is an important carrier of emotion and the easiest way to record information completely. Modern people usually communicate by phone or through APPs. Unfortunately, existing technologies lack standards for speech emotion information. If designed a unified SER framework that automatically recognizes the human emotional state categories (ESC), such as happy, sad, anger, and its emotional intensity scales (EIS) expressed in natural speech, the intelligence systems can better perceive human thoughts and have better interactivity. Recently, deep-learning-based appro","HCI: Chatbots, Affective computing/HCI",TRUE
54,https://arxiv.org/pdf/2302.13729.pdf,"Emotion is one of the most essential characteristics that distinguishes humans from robots [1] and speech is the most basic tool for daily communication [2]. Therefore, analyzing emotion states through speech signals is a continuing concern for the research community.",Affective computing/HCI,TRUE
55,https://pdf.sciencedirectassets.com/271578/1-s2.0-S0167639307X0164X/1-s2.0-S0167639307000830/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIBlrxXaD%2Br6%2FGTwJfV%2FP7G6J9Cc43V6YBU39odt6LNrIAiEArq4D6qnohVlcS5tkt0p%2BzeUi9TI44dRkfRBnZO4P1IcqvAUI2v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDPID9Hj51RNpJ%2Fe7myqQBevypPvpA2uA0zDrTbXO%2By3Ez1arP%2BKbAqnFnwgoY8Xnf2YGLN1U2%2FLx6R3FoLvAlKtu0f1OOh2JbX%2FdXhNwI4sqLhMInFT39DhopGMbYb5em3KUdM5VjfblM9nH6l6G21hYbFKnClmJyHLEtZfXPag6DkE9zLTO0OXrz8idhXkwo5UzzQ%2BW0mJTcLg3QrT%2FTU7u5R9D3qnfeDVG7M%2FEBgN5%2BIRrcJ10u0Y7r0hT8VJwCVUFfcGBR8%2BPrpHnSTtWAneFXtnsVzr8cn1FuTY58iVmle64V45IzNctqqfiq3uEjWphpzf1Bpsh4sk0AzW9PGrcgVrHrbvIu%2FYPrQt3gHBi3%2BEG0%2BU5SjpqrncQRvM0H1a40E8ZJKMo3OHUw%2FwcQgy1oNGR4XWGjSwatCUAIuImpgAXwN4Vylm4C1mri2C4o5eYhjyNo%2FG%2BuUELl6iPxHg%2FjT8dqU7c1Z8DwuIdvMv367BGWgXWeUSuSNjHWRpoWdS02H9na%2BaVw1C5GxAS4w2d67QrjArPUBQHmQVYJQFspUws%2BB%2FE7QDBhvUlVQP5Zh5Ze62aITeDvKAW5lAn5l32tDboYGK35bxOR%2BXNk9%2B59%2FES2P2rEA8szPc34lVxbVFqVEi7cFXmGC5el4qoIXGKnnYUqUJEwAuLTEuoCIfbj%2F6Av2%2FX1c%2FAEt51Boas4Zx991iwG7wdS6bBcnZx%2B9Krq3fzL1fsh7V06MNA%2FYVc%2FQ8BgPpYNxuHqKgC8qcG7D%2BqjoM%2BcYkfXmp%2B5ycGeKpN0AaRGqDothZYzSodRrEDtfWZE6Npm9hWWvJjHqUG%2FitRovVfemmgDbxZ7ex98liI5HOJG0Bl5mmZiI79EBuktWGNJSS0RVEjl8pjFDYSMIOp4rYGOrEBa5ikuakidPMHVTvYSRQxAXPA41O5ENQ6pzJB5L6JqCSsZdRMxY%2BKKX0ctTxF4tIiB6nydcBfLvlu0Q0YnhHWi0h97LWEO7Eh9phl5JD0zu6gijulaJ3ziItvON3vZ8mCw%2FoWNqUpQMlu7ZafrAFHqjTuZJ8vJUjjNCzfgDi%2BEZ80j6ND9PrQDKIArbwNNgvPf60cS3FmH8qQagNYasaLN7nnpklIJ9WmJBIMnqeJkvGm&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240904T180435Z&X-Amz-SignedHeaders=host&X-Amz-Expires=299&X-Amz-Credential=ASIAQ3PHCVTY5MKMAYWO%2F20240904%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b13ac483cff835de57fc3f4fb9a08fc0b0b1bc2859341be7efa63192dd599fc8&hash=a2a445c3fac846e66b286c683993532741e644540c430f7b9c43799c897a6e5a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167639307000830&tid=spdf-fd0d71c5-895d-4c2f-8bb6-d83877967b40&sid=372daaf19cd1054ddc395a71803abde0afc0gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=10145d07030600555d53&rr=8bdfe7213e5238aa&cc=us,"Technological progress has allowed an increasing degree of human–machine interaction. This interaction can be improved and accelerated by means of spoken communication. In human–human speech-based communications, emotions play an important role, sometimes playing an even bigger role than the logical information also included in the speech (Cowie et al., 2001). One important research challenge in the last few years has thus been automatic recognition of the emotional state of a speaker through speech; knowledge of this state can have a series of applications, such as (Cowie et al., 2001): • automatic answering machines that can adapt their voice tone to that of the speaker; • speech recognition systems that can correctly interpret the meaning of words spoken in an ironic or sarcastic manner;• systems which are able to detect the emotional state of
an interlocutor (for clinical diagnosis purposes, for
example);
• synthesizers that can generate speech giving the sensation of a particular emotion;
• automatic tutoring systems that can establish a learner’s
degree of boredom, irritation or intimidation;
• systems that can prevent speakers in a particularly
altered emotional state from interacting with automatic
recognition systems;
• systems to generate alarms on the basis of the different
emotional states of people being monitored;
• entertainment systems and games that can determine a
user’s emotional state.","HCI: Chatbots, Healthcare: Mental health, Healthcare: Neurological disorders, Security, Affective computing/HCI, Other / misc",TRUE
56,https://repositorio.uam.es/bitstream/10486/663471/1/speaker_lopez-moreno_Interspeech_2009.pdf,"Emotion recognition from the speech signal is an increasingly interesting task in human-machine interaction, with diverse applications in the speech technologies field such as call centres, intelligent auto-mobile systems, speaker intra-variability compensation or entertainment industry","HCI: Chatbots, Other / misc",TRUE
57,https://opus.bibliothek.uni-augsburg.de/opus4/files/71714/triantafyllopoulos19_interspeech.pdf,"technical

speech enhancement applied to SER",Prior work,TRUE
58,https://www.isca-archive.org/interspeech_2022/goncalves22_interspeech.html,No motivation - just launches into why the task is challenging & why multimodal info should be used as a pre-text task for SSL even if the downstream task is unimodal,,TRUE
59,http://ieeexplore.ieee.org/document/4218293/,"Emotion recognition is important for human-machine interaction applications. For example, it is a key ingredient in the design of humanoid robots [1], where “emotional intelligence” is being increasingly added to artificial intelligence design [2]. T","HCI: Chatbots, Affective computing/HCI",TRUE
60,http://ieeexplore.ieee.org/document/1415120/,"Detecting emotion from speech can be viewed as a classification task. It consists of assigning, out of a fixed set, an emotion category e.g., joviality, anger, fear, or satisfaction, to a speech utterance. Accurate detection of emotion from speech has clear benefits for the design of more natural human-machine speech interfaces or for the extraction of useful information from large quantities of speech data. It can help design more natural spoken-dialog systems than those currently deployed in call centers or used in tutoring systems. The speaker’s emotion can be exploited by the system’s dialog manager to provide more suitable responses thereby achieving better task completion rates. Emotion detection can also be used to rapidly identify relevant speech
or multimedia documents from a large data set.","HCI: Chatbots, Affective computing/HCI, Other / misc",TRUE
61,https://ieeexplore.ieee.org/ielx7/10445798/10445803/10446272.pdf,"Emotion is a complex process influencing human interaction and cognitive processes [1, 2]. Speech emotion recognition (SER) enables systems to perceive and respond to emotional cues in spoken language and plays a crucial role in human-computer interaction.",Affective computing/HCI,TRUE
62,,The performance of state-of-the-art SER also degrades in the cross-corpus setting when an acoustic mismatch between training and testing exists [4]. This shows that SER systems lack robustness and generalisation which makes them susceptible to unknown test data shifts.,,TRUE
63,,"The first is privacy due to concerns of sensitive information leakage: for example, users may not expect to disclose their identity information while using a speech emotion recognition (SER) system; on the other hand, users may not wish to share their emotional condition when being assessed by a speaker recognition (SR) system. Moreover, the collective social norm would create unwanted and often detrimental self-exaggerated issues around equality, e.g., unfair biases toward gender types [6] or race [7], when using data-driven approaches for speech technology. Speech is an informative signal which contains personal sensitive attributes by nature; hence developing appropriate methods either to protect privacy information, such as identity and emotion, or to mitigate the undesired biases, like gender and race, is critical in the current era.",,TRUE
64,,"For HCI applications, the identification of these factors behind the speech signals, especially the emotion, can enhance the understanding of the user’s intent and improve the experience during the interaction.",,TRUE
65,,"In the context of reliability in real-world applications, SER systems not only need to model ground-truth labels but also account for the subjectivity inherent in these labels [2], [8]",,TRUE
66,,"Especially in the field of human-machine interaction (HCI), growing interest can be observed in recent years. In addition, the detection of lies, monitoring of call centers and medical diagnoses are often claimed as promising application scenarios for speech emotion recognition.",,TRUE
67,,"Most studies on speech emotion recognition are centered on young-adult people, mainly originating from English-speaking countries [3, 4, 5, 6]. This demographic bias causes existing commercial emotion recognition systems to inaccurately perceived emotion in the elderly [7]. Despite the fast-growing elderly demographic in many countries [8], only a few studies with a fairly limited amount of data work on emotion recognition for elderly [9, 10, 11], especially from non-English-speaking countries [12].",,TRUE
68,,"However, most AER systems operate on manually segmented utterances although manual segmentations are not generally available in practical use cases. Besides, automatic speech recognition systems trained on standard speech can give poor recognition performance on emotional speech [",,TRUE
69,https://www.isca-archive.org/interspeech_2023/gao23d_interspeech.html,"HCI – Chatbots
""Understanding the emotional state from speech is crucial for
conversational robots and intelligent devices to generate empathetic responses to the user [1].""
""[1] B. Luo, R. Y. Lau, C. Li, and Y.-W. Si, “A critical review of
state-of-the-art chatbot designs and applications,” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
vol. 12, no. 1, p. e1434, 2022.""",HCI: Chatbots,TRUE
70,https://ieeexplore.ieee.org/document/10094984/,"""Emotion AI is a fast-developing contemporary technology, and the inclusion of a speech emotion recognition (SER) module enables machines to intelligently interact with humans [1, 2].""",HCI: Chatbots,TRUE
71,https://www.pure.ed.ac.uk/ws/files/305860313/SLT_2022_final.pdf,"""With these questions in mind, we study wav2vec 2.0 [17] on emotional corpora, demonstrating how this type of selfsupervised model can be explored for downstream tasks.""",Prior work,TRUE
72,https://arxiv.org/pdf/1904.10788.pdf,"In this era of high-performance computing, human-computer interaction (HCI) has become pervasive. To enrich the user experience, the system is often required to detect human emotion and produce a response with proper emotional context [1, 2].
1] Rosalind W Picard, “Affective computing: challenges,” International Journal of Human-Computer Studies, vol. 59, no. 1-2, pp. 55–64, 2003.
[2] Carlos Busso, Murtaza Bulut, Shrikanth Narayanan, J Gratch, and S Marsella, “Toward effective automatic recognition systems of emotion in speech,” Social emotions in nature and artifact: emotions in human and human-computer interaction, J. Gratch and S. Marsella, Eds, pp. 110–127, 2013.
Beside HCI, the output of emotion recognition engine is beneficial in the paralinguistic area as well [3].
[3] Agata Ko?akowska, Agnieszka Landowska, Mariusz Szwoch,
Wioleta Szwoch, and Michal R Wrobel, “Emotion recognition and its applications,” in Human-Computer Systems Interaction: Backgrounds and Applications 3, pp. 51–62. Springer,
2014.",HCI: Chatbots,TRUE
73,https://www.isca-archive.org/icslp_2002/makarova02_icslp.html,"""Lifelike software agents expand in our computerized working space, while our homes are invaded by robotic interactive toys, such
as Tiger’s Furbies and Sony’s Aibo, who are supposed to be able to express and understand emotions. A new field of research in AI known as affective computing has recently been identified [3].""
[3] Picard, R. Affective computing. The MIT Press. 1997","HCI: Chatbots, Affective computing/HCI",TRUE
74,http://ieeexplore.ieee.org/document/4959521/,"""For example, recognizing affective content of music signals ([3], [4]) can be used in a system, where the users will be able to retrieve musical data with regard to affective content. In a similar way, affective content recognition in video data could be used for retrieving videos that contain specific emotions. In this paper, we emphasize on affective content that can be retrieved from the speech information of movies.""",Other / misc,TRUE
75,https://www.isca-archive.org/interspeech_2018/yang18c_interspeech.html,"""In recent years, increasing attention has been given to the study of the emotional content in speech signals, and many systems have been proposed for automatic emotion recognition in speech.""",Prior work,TRUE
76,,"""However, the applications of SDSs are still limited to simple informational dialog systems, such as navigation systems and air travel information systems [1], [2]. To enable more complex applications (e.g., home nursing [3], educational/tutoring, and chatting [4]), new capabilities such as affective interaction are needed. However, to achieve the goal of affective interaction via speech, several problems in speech technologies, including low accuracy in recognition of highly affective speech and lack of affectrelated common sense and basic knowledge, still exist""","HCI: Chatbots, Other / misc",TRUE
77,https://www.isca-archive.org/interspeech_2023/gao23d_interspeech.html,"Affective computing + Motivation is drawn from prior work rather than the impact of the task itself

general ""it's important"" motivation + past work has found that ""gender"" & other speech features are helpful with other models

not sure if the first background paragraph is saying that using pre-trained features + more basic SER models can show improved performance over just basic SER models alone",HCI: Chatbots,TRUE
78,https://opus.bibliothek.uni-augsburg.de/opus4/files/90748/latif20b_interspeech.pdf,"technical
increase robustness to different audio environments",Prior work,TRUE
79,https://www.pure.ed.ac.uk/ws/files/305860313/SLT_2022_final.pdf,"technical

are features learned by models useful for all downstream tasks?

With these questions in mind, we study wav2vec 2.0 [17]
on emotional corpora, demonstrating how this type of selfsupervised model can be explored for downstream tasks. Our
experiments show that: 1) wav2vec 2.0 appears to discard
some paralinguistic information that is less useful for word
recognition purposes and does not treat all emotions and paralinguistic features equally; 2) for SER, representations from
the final layer could result in the worst performance in some
cases; 3) current self-supervised models need to be carefully
fine-tuned to adapt to downstream tasks that make use of nonlexical features. We hope our findings can provide the research community with a new perspective to look at the effectiveness and usage of self-supervised models",Prior work,TRUE
80,https://arxiv.org/pdf/1904.10788.pdf,"general ""HCI""

To enrich the user experience,
the system is often required to detect human emotion and produce a
response with proper emotional context [1, 2]. The first step in such
an HCI involves building a system that recognizes emotion from the
speech utterance. A speech emotional system aims to identify audio recording as belonging to one of the categories, like happy, sad,
angry or neutral. Beside HCI, the output of emotion recognition engine is beneficial in the paralinguistic area as well",HCI: Chatbots,TRUE
81,https://www.isca-archive.org/icslp_2002/makarova02_icslp.html,"The database
was created with two major objectives. First, it is a tool for
linguistic investigation of emotion expression in some major
intonation contours of Russian. Second, it serves as a source of training and test data needed to create recognition systems for
basic emotions. The database allows designing recognizers for
the following levels of generality: speaker-and-genderindependent, gender-dependent and speaker-dependent.",Prior work,TRUE
82,,"With the exponential growth in available computing power and significant progress in speech technologies, spoken dialogue systems (SDS) have been successfully applied to several domains. However, the applications of SDSs are still limited to simple informational dialog systems, such as navigation systems and air travel information systems [1], [2]. To enable more complex applications (e.g., home nursing [3], educational/tutoring, and chatting [4]), new capabilities such as affective interaction are needed. However, to achieve the goal of affective interaction via speech, several problems in speech technologies, including low accuracy in recognition of highly affective speech and lack of affectrelated common sense and basic knowledge, still exist.","HCI: Chatbots, Affective computing/HCI, Other / misc",TRUE